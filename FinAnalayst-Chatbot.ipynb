{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":12701853,"sourceType":"datasetVersion","datasetId":8027465}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"pip install -q datasets transformers peft accelerate","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-07T16:16:42.595101Z","iopub.execute_input":"2025-08-07T16:16:42.595722Z","iopub.status.idle":"2025-08-07T16:18:12.911874Z","shell.execute_reply.started":"2025-08-07T16:16:42.595698Z","shell.execute_reply":"2025-08-07T16:18:12.910852Z"}},"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m95.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m67.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m30.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m72.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 2.8.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\ncesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\ngcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\nbigframes 2.8.0 requires google-cloud-bigquery[bqstorage,pandas]>=3.31.0, but you have google-cloud-bigquery 3.25.0 which is incompatible.\nbigframes 2.8.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":25},{"cell_type":"code","source":"import json\nfrom pathlib import Path\n\ndef convert_finqa(input_path, output_path):\n    with open(input_path, 'r', encoding='utf-8') as f:\n        raw_data = json.load(f)\n\n    converted = []\n    for item in raw_data:\n        # Soru\n        question = item[\"qa\"][\"question\"]\n        answer = item[\"qa\"].get(\"answer\", \"\")\n\n        # Tabloyu string olarak birleştir\n        table = item.get(\"table\", [])\n        table_text = \"\\n\".join([\" | \".join(row) for row in table])\n\n        # Kaydet\n        converted.append({\n            \"context\": table_text,\n            \"question\": question,\n            \"answer\": answer\n        })\n\n    # JSONL olarak kaydet\n    with open(output_path, 'w', encoding='utf-8') as f:\n        for row in converted:\n            f.write(json.dumps(row, ensure_ascii=False) + '\\n')\n\n# Yollar\nbase_path = \"/kaggle/input/finqa-ds\"\nout_path = \"/kaggle/working\"\n\n# Dönüştürme işlemleri\nconvert_finqa(f\"{base_path}/train.json\", f\"{out_path}/train_converted.jsonl\")\nconvert_finqa(f\"{base_path}/dev.json\", f\"{out_path}/dev_converted.jsonl\")\nconvert_finqa(f\"{base_path}/test.json\", f\"{out_path}/test_converted.jsonl\")\n\nprint(\"✔️ Dönüştürme tamamlandı.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from datasets import load_dataset\n\n# JSONL dosyalarını HuggingFace formatında yükle\ndata_files = {\n    \"train\": \"/kaggle/working/train_converted.jsonl\",\n    \"validation\": \"/kaggle/working/dev_converted.jsonl\",\n    \"test\": \"/kaggle/working/test_converted.jsonl\"\n}\n\nraw_datasets = load_dataset(\"json\", data_files=data_files, split={\"train\": \"train\", \"validation\": \"validation\", \"test\": \"test\"})\n\n# 🧠 Input-output formatına dönüştür\ndef preprocess(example):\n    return {\n        \"input\": f\"Table:\\n{example['context']}\\n\\nQuestion: {example['question']}\",\n        \"output\": example[\"answer\"]\n    }\n\ntokenized_datasets = raw_datasets.map(preprocess)\n\n# Kontrol: Örnek göster\nprint(tokenized_datasets[\"train\"][0])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-07T16:18:49.846331Z","iopub.execute_input":"2025-08-07T16:18:49.847006Z","iopub.status.idle":"2025-08-07T16:18:50.685802Z","shell.execute_reply.started":"2025-08-07T16:18:49.846975Z","shell.execute_reply":"2025-08-07T16:18:50.684909Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Generating train split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f4de178f284e4fdaa85ea1b24fa2d73b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5daa9e52fc6a4153890ee8a1dea99c81"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"320e006f418f44b8b498e04dc0ea3853"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/6251 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"117668043aed4728aed72485fdfa133f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/883 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0a3fe26b29614a34aea3799b8c5c2df5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1147 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0c882eaf874549f69a7ec57ea7075e7d"}},"metadata":{}},{"name":"stdout","text":"{'context': ' | october 31 2009 | november 1 2008\\nfair value of forward exchange contracts asset ( liability ) | $ 6427 | $ -23158 ( 23158 )\\nfair value of forward exchange contracts after a 10% ( 10 % ) unfavorable movement in foreign currency exchange rates asset ( liability ) | $ 20132 | $ -9457 ( 9457 )\\nfair value of forward exchange contracts after a 10% ( 10 % ) favorable movement in foreign currency exchange rates liability | $ -6781 ( 6781 ) | $ -38294 ( 38294 )', 'question': 'what is the the interest expense in 2009?', 'answer': '380', 'input': 'Table:\\n | october 31 2009 | november 1 2008\\nfair value of forward exchange contracts asset ( liability ) | $ 6427 | $ -23158 ( 23158 )\\nfair value of forward exchange contracts after a 10% ( 10 % ) unfavorable movement in foreign currency exchange rates asset ( liability ) | $ 20132 | $ -9457 ( 9457 )\\nfair value of forward exchange contracts after a 10% ( 10 % ) favorable movement in foreign currency exchange rates liability | $ -6781 ( 6781 ) | $ -38294 ( 38294 )\\n\\nQuestion: what is the the interest expense in 2009?', 'output': '380'}\n","output_type":"stream"}],"execution_count":26},{"cell_type":"code","source":"from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\nfrom peft import LoraConfig, get_peft_model, TaskType\n\nmodel_name = \"google/flan-t5-large\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nbase_model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n\npeft_config = LoraConfig(\n    task_type=TaskType.SEQ_2_SEQ_LM,\n    r=16,\n    lora_alpha=32,\n    lora_dropout=0.1,\n    bias=\"none\"\n)\n\nmodel = get_peft_model(base_model, peft_config)\n\nmodel.print_trainable_parameters()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-07T16:19:32.816655Z","iopub.execute_input":"2025-08-07T16:19:32.817000Z","iopub.status.idle":"2025-08-07T16:19:54.026447Z","shell.execute_reply.started":"2025-08-07T16:19:32.816975Z","shell.execute_reply":"2025-08-07T16:19:54.025583Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"03f67524445147c2af37489f1cc86c4f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"96882b64df0c4d19b1498e84de345a46"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"066de183747f413dacf9b58ef403a30e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"82d7dcb4905f4a71b1b7d967b96f27b1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/662 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3a19a87c992d41459a3a80c7419ba1db"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/3.13G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"57089162406349668d52c3ce19e7f088"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e655833f218b4395930e1adef49d0dd3"}},"metadata":{}},{"name":"stdout","text":"trainable params: 4,718,592 || all params: 787,868,672 || trainable%: 0.5989\n","output_type":"stream"}],"execution_count":27},{"cell_type":"code","source":"def tokenize(sample):\n    model_input = tokenizer(\n        sample[\"input\"],\n        max_length=256,        \n        padding=\"max_length\",\n        truncation=True,\n    )\n    labels = tokenizer(\n        sample[\"output\"],\n        max_length=64,         \n        padding=\"max_length\",\n        truncation=True\n    )\n    model_input[\"labels\"] = labels[\"input_ids\"]\n    return model_input\n\n\n# Tokenize işlemi\ntokenized_dataset = tokenized_datasets.map(tokenize, batched=True, remove_columns=tokenized_datasets[\"train\"].column_names)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-07T16:24:48.034178Z","iopub.execute_input":"2025-08-07T16:24:48.034486Z","iopub.status.idle":"2025-08-07T16:24:49.667037Z","shell.execute_reply.started":"2025-08-07T16:24:48.034467Z","shell.execute_reply":"2025-08-07T16:24:49.666461Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/6251 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"68f3a3722b6f47749070ccb38cc3beb9"}},"metadata":{}}],"execution_count":37},{"cell_type":"code","source":"from transformers import TrainingArguments, Trainer, DataCollatorForSeq2Seq\nimport torch\n\n# GPU varsa otomatik kullan\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\n\n# Veri toplayıcı (otomatik padding ve label shifting için)\ndata_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-07T16:22:30.440890Z","iopub.execute_input":"2025-08-07T16:22:30.441204Z","iopub.status.idle":"2025-08-07T16:22:31.605342Z","shell.execute_reply.started":"2025-08-07T16:22:30.441182Z","shell.execute_reply":"2025-08-07T16:22:31.604455Z"}},"outputs":[],"execution_count":29},{"cell_type":"code","source":"training_args = TrainingArguments(\n    output_dir=\"/kaggle/working/finbot-lora-checkpoints\",\n    per_device_train_batch_size=1,        \n    per_device_eval_batch_size=1,           \n    gradient_accumulation_steps=8,         \n    num_train_epochs=3,\n    learning_rate=5e-5,\n    eval_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    logging_strategy=\"steps\",\n    logging_steps=100,\n    save_total_limit=2,\n    fp16=False,                         \n    report_to=\"none\",\n    push_to_hub=False\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-07T16:28:17.550248Z","iopub.execute_input":"2025-08-07T16:28:17.550548Z","iopub.status.idle":"2025-08-07T16:28:17.585173Z","shell.execute_reply.started":"2025-08-07T16:28:17.550530Z","shell.execute_reply":"2025-08-07T16:28:17.584625Z"}},"outputs":[],"execution_count":40},{"cell_type":"code","source":"trainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_dataset[\"train\"],\n    eval_dataset=tokenized_dataset[\"validation\"],\n    tokenizer=tokenizer,\n    data_collator=data_collator\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-07T16:28:18.606283Z","iopub.execute_input":"2025-08-07T16:28:18.606890Z","iopub.status.idle":"2025-08-07T16:28:18.634665Z","shell.execute_reply.started":"2025-08-07T16:28:18.606868Z","shell.execute_reply":"2025-08-07T16:28:18.633974Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_36/1815454466.py:1: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\nNo label_names provided for model class `PeftModelForSeq2SeqLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n","output_type":"stream"}],"execution_count":41},{"cell_type":"code","source":"import torch\ntorch.cuda.empty_cache()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-07T16:28:19.347644Z","iopub.execute_input":"2025-08-07T16:28:19.347881Z","iopub.status.idle":"2025-08-07T16:28:19.351847Z","shell.execute_reply.started":"2025-08-07T16:28:19.347865Z","shell.execute_reply":"2025-08-07T16:28:19.351288Z"}},"outputs":[],"execution_count":42},{"cell_type":"code","source":"trainer.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-07T16:28:21.341645Z","iopub.execute_input":"2025-08-07T16:28:21.342171Z","iopub.status.idle":"2025-08-07T19:20:48.293315Z","shell.execute_reply.started":"2025-08-07T16:28:21.342147Z","shell.execute_reply":"2025-08-07T19:20:48.292706Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1173' max='1173' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1173/1173 2:52:18, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>13.962200</td>\n      <td>5.850568</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>2.320000</td>\n      <td>0.296526</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.294000</td>\n      <td>0.252665</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"execution_count":43,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=1173, training_loss=7.753166102793922, metrics={'train_runtime': 10346.4196, 'train_samples_per_second': 1.813, 'train_steps_per_second': 0.113, 'total_flos': 2.174657388399821e+16, 'train_loss': 7.753166102793922, 'epoch': 3.0})"},"metadata":{}}],"execution_count":43},{"cell_type":"code","source":"save_path = \"/kaggle/working/finbot-peft-model\"\n\nmodel.save_pretrained(save_path)\ntokenizer.save_pretrained(save_path)\nprint(f\"Model ve tokenizer '{save_path}' klasörüne kaydedildi.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-07T19:27:20.090638Z","iopub.execute_input":"2025-08-07T19:27:20.091242Z","iopub.status.idle":"2025-08-07T19:27:20.380715Z","shell.execute_reply.started":"2025-08-07T19:27:20.091217Z","shell.execute_reply":"2025-08-07T19:27:20.380076Z"}},"outputs":[{"name":"stdout","text":"Model ve tokenizer '/kaggle/working/finbot-peft-model' klasörüne kaydedildi.\n","output_type":"stream"}],"execution_count":44},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}